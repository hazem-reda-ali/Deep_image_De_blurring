{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab-DeblurGANv2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1Cyaut6utnv",
        "colab_type": "text"
      },
      "source": [
        "# Colab-DeblurGANv2\n",
        "\n",
        "Currently only one picture as input allowed. \n",
        "\n",
        "Original repo: [TAMU-VITA/DeblurGANv2](https://github.com/TAMU-VITA/DeblurGANv2)\n",
        "\n",
        "My coalb fork: \n",
        "\n",
        "Simple tutoiral:\n",
        "Place ```input.png``` in ```Google Drive/DeblurGanv2``` and run these cells. You can create this folder with colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUTg2s-by8si",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "42de5d4b-b176-4ab2-e2f3-27a91432ddab"
      },
      "source": [
        "# check gpu\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Jul  4 23:44:43 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.36.06    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P8     8W /  75W |      0MiB /  7611MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mb3vhUOn5SzO",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "910177c3-b450-45ab-e817-4f752589e655"
      },
      "source": [
        "#@title Connect Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print('Google Drive connected.')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "Google Drive connected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9OIL78n5p2E",
        "colab_type": "text"
      },
      "source": [
        "Either create input and output folders manually or with colab and place the files there"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4FJ_R7d5W2a",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title [Optional] Creating empty ```\"/content/drive/My Drive/Colab-DeblurGanv2/\"``` folder in Google Drive\n",
        "!mkdir \"/content/drive/My Drive/Colab-DeblurGanv2/\"\n",
        "#!mkdir \"/content/drive/My Drive/Colab-DeblurGanv2/input/\"\n",
        "#!mkdir \"/content/drive/My Drive/Colab-DeblurGanv2/output/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQbbcY7Ouzd_",
        "colab_type": "text"
      },
      "source": [
        "# Deblur with InceptionResNet-v2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4G9sFBtrnmZz",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "outputId": "4546950d-3ae6-4227-f277-4e33f933a55c"
      },
      "source": [
        "#@title Installing and downloading model\n",
        "%cd /content/\n",
        "!git clone https://github.com/styler00dollar/Colab-DeblurGANv2\n",
        "!pip install fire\n",
        "!pip install pretrainedmodels\n",
        "!pip install gdown\n",
        "%cd /content/Colab-DeblurGANv2/models\n",
        "!gdown --id 1UXcsRVW-6KF23_TNzxw-xC0SzaMfXOaR"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'Colab-DeblurGANv2'...\n",
            "remote: Enumerating objects: 22, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 817 (delta 9), reused 9 (delta 3), pack-reused 795\u001b[K\n",
            "Receiving objects: 100% (817/817), 61.78 MiB | 9.36 MiB/s, done.\n",
            "Resolving deltas: 100% (423/423), done.\n",
            "Collecting fire\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/a7/0e22e70778aca01a52b9c899d9c145c6396d7b613719cd63db97ffa13f2f/fire-0.3.1.tar.gz (81kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 5.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire) (1.12.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire) (1.1.0)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.3.1-py2.py3-none-any.whl size=111005 sha256=5a6278d11ae4a96e92a436bcd12783922916998d26a0ca5f1bd803c8436f12ee\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/61/df/768b03527bf006b546dce284eb4249b185669e65afc5fbb2ac\n",
            "Successfully built fire\n",
            "Installing collected packages: fire\n",
            "Successfully installed fire-0.3.1\n",
            "Collecting pretrainedmodels\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/0e/be6a0e58447ac16c938799d49bfb5fb7a80ac35e137547fc6cee2c08c4cf/pretrainedmodels-0.7.4.tar.gz (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 4.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (1.5.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (0.6.1+cu101)\n",
            "Collecting munch\n",
            "  Downloading https://files.pythonhosted.org/packages/cc/ab/85d8da5c9a45e072301beb37ad7f833cd344e04c817d97e0cc75681d248f/munch-2.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (4.41.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->pretrainedmodels) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->pretrainedmodels) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->pretrainedmodels) (7.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from munch->pretrainedmodels) (1.12.0)\n",
            "Building wheels for collected packages: pretrainedmodels\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-cp36-none-any.whl size=60962 sha256=bf4706f3631cfb535e8eff5745689883e90dc9063b11bb206c83233a334d5069\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/df/63/62583c096289713f22db605aa2334de5b591d59861a02c2ecd\n",
            "Successfully built pretrainedmodels\n",
            "Installing collected packages: munch, pretrainedmodels\n",
            "Successfully installed munch-2.5.0 pretrainedmodels-0.7.4\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (3.6.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gdown) (1.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gdown) (4.41.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (3.0.4)\n",
            "/content/Colab-DeblurGANv2/models\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1UXcsRVW-6KF23_TNzxw-xC0SzaMfXOaR\n",
            "To: /content/Colab-DeblurGANv2/models/fpn_inception.h5\n",
            "244MB [00:01, 200MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PW6au4DWuJkV",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "501810ed-96d4-467a-ae65-2e55ff0f25a7"
      },
      "source": [
        "#@title Changing predict.py\n",
        "%%writefile /content/Colab-DeblurGANv2/predict.py\n",
        "import os\n",
        "from glob import glob\n",
        "from typing import Optional\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import yaml\n",
        "from fire import Fire\n",
        "from tqdm import tqdm\n",
        "\n",
        "from aug import get_normalize\n",
        "from models.networks import get_generator\n",
        "\n",
        "\n",
        "class Predictor:\n",
        "    def __init__(self, weights_path: str, model_name: str = ''):\n",
        "        with open('config/config.yaml') as cfg:\n",
        "            config = yaml.load(cfg)\n",
        "        model = get_generator(model_name or config['model'])\n",
        "        model.load_state_dict(torch.load(weights_path)['model'])\n",
        "        self.model = model.cuda()\n",
        "        self.model.train(True)\n",
        "        # GAN inference should be in train mode to use actual stats in norm layers,\n",
        "        # it's not a bug\n",
        "        self.normalize_fn = get_normalize()\n",
        "\n",
        "    @staticmethod\n",
        "    def _array_to_batch(x):\n",
        "        x = np.transpose(x, (2, 0, 1))\n",
        "        x = np.expand_dims(x, 0)\n",
        "        return torch.from_numpy(x)\n",
        "\n",
        "    def _preprocess(self, x: np.ndarray, mask: Optional[np.ndarray]):\n",
        "        x, _ = self.normalize_fn(x, x)\n",
        "        if mask is None:\n",
        "            mask = np.ones_like(x, dtype=np.float32)\n",
        "        else:\n",
        "            mask = np.round(mask.astype('float32') / 255)\n",
        "\n",
        "        h, w, _ = x.shape\n",
        "        block_size = 32\n",
        "        min_height = (h // block_size + 1) * block_size\n",
        "        min_width = (w // block_size + 1) * block_size\n",
        "\n",
        "        pad_params = {'mode': 'constant',\n",
        "                      'constant_values': 0,\n",
        "                      'pad_width': ((0, min_height - h), (0, min_width - w), (0, 0))\n",
        "                      }\n",
        "        x = np.pad(x, **pad_params)\n",
        "        mask = np.pad(mask, **pad_params)\n",
        "\n",
        "        return map(self._array_to_batch, (x, mask)), h, w\n",
        "\n",
        "    @staticmethod\n",
        "    def _postprocess(x: torch.Tensor) -> np.ndarray:\n",
        "        x, = x\n",
        "        x = x.detach().cpu().float().numpy()\n",
        "        x = (np.transpose(x, (1, 2, 0)) + 1) / 2.0 * 255.0\n",
        "        return x.astype('uint8')\n",
        "\n",
        "    def __call__(self, img: np.ndarray, mask: Optional[np.ndarray], ignore_mask=True) -> np.ndarray:\n",
        "        (img, mask), h, w = self._preprocess(img, mask)\n",
        "        with torch.no_grad():\n",
        "            inputs = [img.cuda()]\n",
        "            if not ignore_mask:\n",
        "                inputs += [mask]\n",
        "            pred = self.model(*inputs)\n",
        "        return self._postprocess(pred)[:h, :w, :]\n",
        "\n",
        "def process_video(pairs, predictor, output_dir):\n",
        "    for video_filepath, mask in tqdm(pairs):\n",
        "        video_filename = os.path.basename(video_filepath)\n",
        "        output_filepath = os.path.join(output_dir, os.path.splitext(video_filename)[0]+'_deblur.mp4')\n",
        "        video_in = cv2.VideoCapture(video_filepath)\n",
        "        fps = video_in.get(cv2.CAP_PROP_FPS)\n",
        "        width = int(video_in.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(video_in.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        total_frame_num = int(video_in.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        video_out = cv2.VideoWriter(output_filepath, cv2.VideoWriter_fourcc(*'MP4V'), fps, (width, height))\n",
        "        tqdm.write(f'process {video_filepath} to {output_filepath}, {fps}fps, resolution: {width}x{height}')\n",
        "        for frame_num in tqdm(range(total_frame_num), desc=video_filename):\n",
        "            res, img = video_in.read()\n",
        "            if not res:\n",
        "                break\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            pred = predictor(img, mask)\n",
        "            pred = cv2.cvtColor(pred, cv2.COLOR_RGB2BGR)\n",
        "            video_out.write(pred)\n",
        "\n",
        "def main(img_pattern: str,\n",
        "         mask_pattern: Optional[str] = None,\n",
        "         weights_path='/content/Colab-DeblurGANv2/models/fpn_inception.h5',\n",
        "         out_dir='submit/',\n",
        "         side_by_side: bool = False,\n",
        "         video: bool = False):\n",
        "    def sorted_glob(pattern):\n",
        "        return sorted(glob(pattern))\n",
        "\n",
        "    imgs = sorted_glob(img_pattern)\n",
        "    masks = sorted_glob(mask_pattern) if mask_pattern is not None else [None for _ in imgs]\n",
        "    pairs = zip(imgs, masks)\n",
        "    names = sorted([os.path.basename(x) for x in glob(img_pattern)])\n",
        "    predictor = Predictor(weights_path=weights_path)\n",
        "\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    if not video:\n",
        "        for name, pair in tqdm(zip(names, pairs), total=len(names)):\n",
        "            f_img, f_mask = pair\n",
        "            img, mask = map(cv2.imread, (f_img, f_mask))\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            pred = predictor(img, mask)\n",
        "            if side_by_side:\n",
        "                pred = np.hstack((img, pred))\n",
        "            pred = cv2.cvtColor(pred, cv2.COLOR_RGB2BGR)\n",
        "            cv2.imwrite(os.path.join(out_dir, name),\n",
        "                        pred)\n",
        "    else:\n",
        "        process_video(pairs, predictor, out_dir)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    Fire(main)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting /content/Colab-DeblurGANv2/predict.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lImGMorGKGTd",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "6f15b200-3513-49b1-89c6-a3f7fea4578d"
      },
      "source": [
        "#@title Downloading model from my Google Drive (Very fast download)\n",
        "!mkdir /root/\n",
        "!mkdir /root/.cache/\n",
        "!mkdir /root/.cache/torch/\n",
        "!mkdir /root/.cache/torch/checkpoints/\n",
        "%cd /root/.cache/torch/checkpoints/\n",
        "!gdown --id 1y6GeaoWjhqjRjrXuZvCYEQYlblZGkE6X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/root/’: File exists\n",
            "mkdir: cannot create directory ‘/root/.cache/’: File exists\n",
            "mkdir: cannot create directory ‘/root/.cache/torch/’: File exists\n",
            "mkdir: cannot create directory ‘/root/.cache/torch/checkpoints/’: File exists\n",
            "/root/.cache/torch/checkpoints\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1y6GeaoWjhqjRjrXuZvCYEQYlblZGkE6X\n",
            "To: /root/.cache/torch/checkpoints/inceptionresnetv2-520b38e4.pth\n",
            "224MB [00:00, 245MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUQxom6W4YDw",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "f1827431-623e-42de-c8b8-11ccaa3ed34d"
      },
      "source": [
        "#@title ```[OPTIONAL / ONLY RUN THIS IF YOU HAVE PROBLEMS WITH THE PREVIOUS CELL]``` Testrun to download model file with pretrainedmodels (Download takes around 10 minutes)\n",
        "%cd /content/\n",
        "!wget --no-check-certificate https://raw.githubusercontent.com/xinntao/ESRGAN/master/LR/baboon.png\n",
        "%cd /content/Colab-DeblurGANv2\n",
        "!python predict.py /content/baboon.png\n",
        "%cd /content/\n",
        "!sudo rm /content/baboon.png\n",
        "!sudo rm /content/Colab-DeblurGANv2/submit/baboon.png"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "--2020-07-04 23:04:41--  https://raw.githubusercontent.com/xinntao/ESRGAN/master/LR/baboon.png\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 33918 (33K) [image/png]\n",
            "Saving to: ‘baboon.png’\n",
            "\n",
            "baboon.png          100%[===================>]  33.12K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2020-07-04 23:04:42 (8.63 MB/s) - ‘baboon.png’ saved [33918/33918]\n",
            "\n",
            "/content/DeblurGANv2\n",
            "Downloading: \"http://data.lip6.fr/cadene/pretrainedmodels/inceptionresnetv2-520b38e4.pth\" to /root/.cache/torch/checkpoints/inceptionresnetv2-520b38e4.pth\n",
            "100% 213M/213M [10:28<00:00, 356kB/s]\n",
            "  0% 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2796: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
            "100% 1/1 [00:00<00:00,  2.34it/s]\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kImTgk3pncs",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "aeddc653-be26-4c7d-db8d-b0af90b8fc6d"
      },
      "source": [
        "#@title Run deblur and copy result go Google Drive\n",
        "%cd /content/Colab-DeblurGANv2\n",
        "!python predict.py \"/content/drive/My Drive/Colab-DeblurGanv2/input.png\"\n",
        "!cp /content/Colab-DeblurGANv2/submit/input.png \"/content/drive/My Drive/Colab-DeblurGanv2/output.png\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Colab-DeblurGANv2\n",
            "  0% 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2796: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
            "100% 1/1 [00:01<00:00,  1.25s/it]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Qucha-Su4J7",
        "colab_type": "text"
      },
      "source": [
        "# Deblur with MobileNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZmeIqZ8u8YL",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "eb3ade92-a729-4109-bf3b-962c4d0e1c6d"
      },
      "source": [
        "#@title Installing and downloading models\n",
        "%cd /content/\n",
        "!git clone https://github.com/styler00dollar/Colab-DeblurGANv2\n",
        "!pip install fire\n",
        "!pip install pretrainedmodels\n",
        "!pip install gdown\n",
        "%cd /content/Colab-DeblurGANv2/models\n",
        "!gdown --id 1UXcsRVW-6KF23_TNzxw-xC0SzaMfXOaR\n",
        "!wget --no-check-certificate http://sceneparsing.csail.mit.edu/model/pretrained_resnet/mobilenet_v2.pth.tar\n",
        "!gdown --id 1JhnT4BBeKBBSLqTo6UsJ13HeBXevarrU"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'Colab-DeblurGANv2'...\n",
            "remote: Enumerating objects: 22, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 817 (delta 9), reused 9 (delta 3), pack-reused 795\u001b[K\n",
            "Receiving objects: 100% (817/817), 61.78 MiB | 18.83 MiB/s, done.\n",
            "Resolving deltas: 100% (423/423), done.\n",
            "Collecting fire\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/a7/0e22e70778aca01a52b9c899d9c145c6396d7b613719cd63db97ffa13f2f/fire-0.3.1.tar.gz (81kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 5.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire) (1.12.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire) (1.1.0)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.3.1-py2.py3-none-any.whl size=111005 sha256=8e1324c09ee0cd67a223591bf55f30eecf68b24677d91fc7a0cae52d36239934\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/61/df/768b03527bf006b546dce284eb4249b185669e65afc5fbb2ac\n",
            "Successfully built fire\n",
            "Installing collected packages: fire\n",
            "Successfully installed fire-0.3.1\n",
            "Collecting pretrainedmodels\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/0e/be6a0e58447ac16c938799d49bfb5fb7a80ac35e137547fc6cee2c08c4cf/pretrainedmodels-0.7.4.tar.gz (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (1.5.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (0.6.1+cu101)\n",
            "Collecting munch\n",
            "  Downloading https://files.pythonhosted.org/packages/cc/ab/85d8da5c9a45e072301beb37ad7f833cd344e04c817d97e0cc75681d248f/munch-2.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (4.41.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->pretrainedmodels) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->pretrainedmodels) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->pretrainedmodels) (7.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from munch->pretrainedmodels) (1.12.0)\n",
            "Building wheels for collected packages: pretrainedmodels\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-cp36-none-any.whl size=60962 sha256=8655dd6d470589d32cb621a5b386dc3274b6579e634116b3c39c7ae6bb5de514\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/df/63/62583c096289713f22db605aa2334de5b591d59861a02c2ecd\n",
            "Successfully built pretrainedmodels\n",
            "Installing collected packages: munch, pretrainedmodels\n",
            "Successfully installed munch-2.5.0 pretrainedmodels-0.7.4\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (3.6.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gdown) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gdown) (1.12.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (3.0.4)\n",
            "/content/Colab-DeblurGANv2/models\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1UXcsRVW-6KF23_TNzxw-xC0SzaMfXOaR\n",
            "To: /content/Colab-DeblurGANv2/models/fpn_inception.h5\n",
            "244MB [00:01, 210MB/s]\n",
            "--2020-07-04 23:53:39--  http://sceneparsing.csail.mit.edu/model/pretrained_resnet/mobilenet_v2.pth.tar\n",
            "Resolving sceneparsing.csail.mit.edu (sceneparsing.csail.mit.edu)... 128.30.100.255\n",
            "Connecting to sceneparsing.csail.mit.edu (sceneparsing.csail.mit.edu)|128.30.100.255|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14205652 (14M) [application/x-tar]\n",
            "Saving to: ‘mobilenet_v2.pth.tar’\n",
            "\n",
            "mobilenet_v2.pth.ta 100%[===================>]  13.55M  10.7MB/s    in 1.3s    \n",
            "\n",
            "2020-07-04 23:53:41 (10.7 MB/s) - ‘mobilenet_v2.pth.tar’ saved [14205652/14205652]\n",
            "\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1JhnT4BBeKBBSLqTo6UsJ13HeBXevarrU\n",
            "To: /content/Colab-DeblurGANv2/models/fpn_mobilenet.h5\n",
            "13.5MB [00:00, 51.1MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-xu__FIp6Jz",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "670b056a-2ea2-47c7-a6d2-4401e6ba6b8a"
      },
      "source": [
        "#@title Changing config.yaml\n",
        "%%writefile /content/Colab-DeblurGANv2/config/config.yaml\n",
        "---\n",
        "project: deblur_gan\n",
        "experiment_desc: fpn\n",
        "\n",
        "train:\n",
        "  files_a: &FILES_A /datasets/my_dataset/**/*.jpg\n",
        "  files_b: *FILES_A\n",
        "  size: &SIZE 256\n",
        "  crop: random\n",
        "  preload: &PRELOAD false\n",
        "  preload_size: &PRELOAD_SIZE 0\n",
        "  bounds: [0, .9]\n",
        "  scope: geometric\n",
        "  corrupt: &CORRUPT\n",
        "    - name: cutout\n",
        "      prob: 0.5\n",
        "      num_holes: 3\n",
        "      max_h_size: 25\n",
        "      max_w_size: 25\n",
        "    - name: jpeg\n",
        "      quality_lower: 70\n",
        "      quality_upper: 90\n",
        "    - name: motion_blur\n",
        "    - name: median_blur\n",
        "    - name: gamma\n",
        "    - name: rgb_shift\n",
        "    - name: hsv_shift\n",
        "    - name: sharpen\n",
        "\n",
        "val:\n",
        "  files_a: *FILES_A\n",
        "  files_b: *FILES_A\n",
        "  size: *SIZE\n",
        "  scope: geometric\n",
        "  crop: center\n",
        "  preload: *PRELOAD\n",
        "  preload_size: *PRELOAD_SIZE\n",
        "  bounds: [.9, 1]\n",
        "  corrupt: *CORRUPT\n",
        "\n",
        "phase: train\n",
        "warmup_num: 3\n",
        "model:\n",
        "  g_name: fpn_mobilenet\n",
        "  blocks: 9\n",
        "  d_name: double_gan # may be no_gan, patch_gan, double_gan, multi_scale\n",
        "  d_layers: 3\n",
        "  content_loss: perceptual\n",
        "  adv_lambda: 0.001\n",
        "  disc_loss: wgan-gp\n",
        "  learn_residual: True\n",
        "  norm_layer: instance\n",
        "  dropout: True\n",
        "\n",
        "num_epochs: 200\n",
        "train_batches_per_epoch: 1000\n",
        "val_batches_per_epoch: 100\n",
        "batch_size: 1\n",
        "image_size: [256, 256]\n",
        "\n",
        "optimizer:\n",
        "  name: adam\n",
        "  lr: 0.0001\n",
        "scheduler:\n",
        "  name: linear\n",
        "  start_epoch: 50\n",
        "  min_lr: 0.0000001"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting /content/Colab-DeblurGANv2/config/config.yaml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOarMZ1Kvjgv",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "332a2f0e-f118-4ef6-ef03-7ff853a342a4"
      },
      "source": [
        "#@title Changing predict.py\n",
        "%%writefile /content/Colab-DeblurGANv2/predict.py\n",
        "import os\n",
        "from glob import glob\n",
        "from typing import Optional\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import yaml\n",
        "from fire import Fire\n",
        "from tqdm import tqdm\n",
        "\n",
        "from aug import get_normalize\n",
        "from models.networks import get_generator\n",
        "\n",
        "\n",
        "class Predictor:\n",
        "    def __init__(self, weights_path: str, model_name: str = ''):\n",
        "        with open('config/config.yaml') as cfg:\n",
        "            config = yaml.load(cfg)\n",
        "        model = get_generator(model_name or config['model'])\n",
        "        model.load_state_dict(torch.load(weights_path)['model'])\n",
        "        self.model = model.cuda()\n",
        "        self.model.train(True)\n",
        "        # GAN inference should be in train mode to use actual stats in norm layers,\n",
        "        # it's not a bug\n",
        "        self.normalize_fn = get_normalize()\n",
        "\n",
        "    @staticmethod\n",
        "    def _array_to_batch(x):\n",
        "        x = np.transpose(x, (2, 0, 1))\n",
        "        x = np.expand_dims(x, 0)\n",
        "        return torch.from_numpy(x)\n",
        "\n",
        "    def _preprocess(self, x: np.ndarray, mask: Optional[np.ndarray]):\n",
        "        x, _ = self.normalize_fn(x, x)\n",
        "        if mask is None:\n",
        "            mask = np.ones_like(x, dtype=np.float32)\n",
        "        else:\n",
        "            mask = np.round(mask.astype('float32') / 255)\n",
        "\n",
        "        h, w, _ = x.shape\n",
        "        block_size = 32\n",
        "        min_height = (h // block_size + 1) * block_size\n",
        "        min_width = (w // block_size + 1) * block_size\n",
        "\n",
        "        pad_params = {'mode': 'constant',\n",
        "                      'constant_values': 0,\n",
        "                      'pad_width': ((0, min_height - h), (0, min_width - w), (0, 0))\n",
        "                      }\n",
        "        x = np.pad(x, **pad_params)\n",
        "        mask = np.pad(mask, **pad_params)\n",
        "\n",
        "        return map(self._array_to_batch, (x, mask)), h, w\n",
        "\n",
        "    @staticmethod\n",
        "    def _postprocess(x: torch.Tensor) -> np.ndarray:\n",
        "        x, = x\n",
        "        x = x.detach().cpu().float().numpy()\n",
        "        x = (np.transpose(x, (1, 2, 0)) + 1) / 2.0 * 255.0\n",
        "        return x.astype('uint8')\n",
        "\n",
        "    def __call__(self, img: np.ndarray, mask: Optional[np.ndarray], ignore_mask=True) -> np.ndarray:\n",
        "        (img, mask), h, w = self._preprocess(img, mask)\n",
        "        with torch.no_grad():\n",
        "            inputs = [img.cuda()]\n",
        "            if not ignore_mask:\n",
        "                inputs += [mask]\n",
        "            pred = self.model(*inputs)\n",
        "        return self._postprocess(pred)[:h, :w, :]\n",
        "\n",
        "def process_video(pairs, predictor, output_dir):\n",
        "    for video_filepath, mask in tqdm(pairs):\n",
        "        video_filename = os.path.basename(video_filepath)\n",
        "        output_filepath = os.path.join(output_dir, os.path.splitext(video_filename)[0]+'_deblur.mp4')\n",
        "        video_in = cv2.VideoCapture(video_filepath)\n",
        "        fps = video_in.get(cv2.CAP_PROP_FPS)\n",
        "        width = int(video_in.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(video_in.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        total_frame_num = int(video_in.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        video_out = cv2.VideoWriter(output_filepath, cv2.VideoWriter_fourcc(*'MP4V'), fps, (width, height))\n",
        "        tqdm.write(f'process {video_filepath} to {output_filepath}, {fps}fps, resolution: {width}x{height}')\n",
        "        for frame_num in tqdm(range(total_frame_num), desc=video_filename):\n",
        "            res, img = video_in.read()\n",
        "            if not res:\n",
        "                break\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            pred = predictor(img, mask)\n",
        "            pred = cv2.cvtColor(pred, cv2.COLOR_RGB2BGR)\n",
        "            video_out.write(pred)\n",
        "\n",
        "def main(img_pattern: str,\n",
        "         mask_pattern: Optional[str] = None,\n",
        "         weights_path='/content/Colab-DeblurGANv2/models/fpn_mobilenet.h5',\n",
        "         out_dir='submit/',\n",
        "         side_by_side: bool = False,\n",
        "         video: bool = False):\n",
        "    def sorted_glob(pattern):\n",
        "        return sorted(glob(pattern))\n",
        "\n",
        "    imgs = sorted_glob(img_pattern)\n",
        "    masks = sorted_glob(mask_pattern) if mask_pattern is not None else [None for _ in imgs]\n",
        "    pairs = zip(imgs, masks)\n",
        "    names = sorted([os.path.basename(x) for x in glob(img_pattern)])\n",
        "    predictor = Predictor(weights_path=weights_path)\n",
        "\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    if not video:\n",
        "        for name, pair in tqdm(zip(names, pairs), total=len(names)):\n",
        "            f_img, f_mask = pair\n",
        "            img, mask = map(cv2.imread, (f_img, f_mask))\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            pred = predictor(img, mask)\n",
        "            if side_by_side:\n",
        "                pred = np.hstack((img, pred))\n",
        "            pred = cv2.cvtColor(pred, cv2.COLOR_RGB2BGR)\n",
        "            cv2.imwrite(os.path.join(out_dir, name),\n",
        "                        pred)\n",
        "    else:\n",
        "        process_video(pairs, predictor, out_dir)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    Fire(main)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting /content/Colab-DeblurGANv2/predict.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9tNNNpKxiNG",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "95ecb08d-f1ca-4a68-f1b8-176f7bd01f2a"
      },
      "source": [
        "#@title Changing fpn_mobilenet.py\n",
        "%%writefile /content/Colab-DeblurGANv2/models/fpn_mobilenet.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from models.mobilenet_v2 import MobileNetV2\n",
        "\n",
        "class FPNHead(nn.Module):\n",
        "    def __init__(self, num_in, num_mid, num_out):\n",
        "        super().__init__()\n",
        "\n",
        "        self.block0 = nn.Conv2d(num_in, num_mid, kernel_size=3, padding=1, bias=False)\n",
        "        self.block1 = nn.Conv2d(num_mid, num_out, kernel_size=3, padding=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.functional.relu(self.block0(x), inplace=True)\n",
        "        x = nn.functional.relu(self.block1(x), inplace=True)\n",
        "        return x\n",
        "\n",
        "\n",
        "class FPNMobileNet(nn.Module):\n",
        "\n",
        "    def __init__(self, norm_layer, output_ch=3, num_filters=64, num_filters_fpn=128, pretrained=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # Feature Pyramid Network (FPN) with four feature maps of resolutions\n",
        "        # 1/4, 1/8, 1/16, 1/32 and `num_filters` filters for all feature maps.\n",
        "\n",
        "        self.fpn = FPN(num_filters=num_filters_fpn, norm_layer = norm_layer, pretrained=pretrained)\n",
        "\n",
        "        # The segmentation heads on top of the FPN\n",
        "\n",
        "        self.head1 = FPNHead(num_filters_fpn, num_filters, num_filters)\n",
        "        self.head2 = FPNHead(num_filters_fpn, num_filters, num_filters)\n",
        "        self.head3 = FPNHead(num_filters_fpn, num_filters, num_filters)\n",
        "        self.head4 = FPNHead(num_filters_fpn, num_filters, num_filters)\n",
        "\n",
        "        self.smooth = nn.Sequential(\n",
        "            nn.Conv2d(4 * num_filters, num_filters, kernel_size=3, padding=1),\n",
        "            norm_layer(num_filters),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.smooth2 = nn.Sequential(\n",
        "            nn.Conv2d(num_filters, num_filters // 2, kernel_size=3, padding=1),\n",
        "            norm_layer(num_filters // 2),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.final = nn.Conv2d(num_filters // 2, output_ch, kernel_size=3, padding=1)\n",
        "\n",
        "    def unfreeze(self):\n",
        "        self.fpn.unfreeze()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        map0, map1, map2, map3, map4 = self.fpn(x)\n",
        "\n",
        "        map4 = nn.functional.upsample(self.head4(map4), scale_factor=8, mode=\"nearest\")\n",
        "        map3 = nn.functional.upsample(self.head3(map3), scale_factor=4, mode=\"nearest\")\n",
        "        map2 = nn.functional.upsample(self.head2(map2), scale_factor=2, mode=\"nearest\")\n",
        "        map1 = nn.functional.upsample(self.head1(map1), scale_factor=1, mode=\"nearest\")\n",
        "\n",
        "        smoothed = self.smooth(torch.cat([map4, map3, map2, map1], dim=1))\n",
        "        smoothed = nn.functional.upsample(smoothed, scale_factor=2, mode=\"nearest\")\n",
        "        smoothed = self.smooth2(smoothed + map0)\n",
        "        smoothed = nn.functional.upsample(smoothed, scale_factor=2, mode=\"nearest\")\n",
        "\n",
        "        final = self.final(smoothed)\n",
        "        res = torch.tanh(final) + x\n",
        "\n",
        "        return torch.clamp(res, min=-1, max=1)\n",
        "\n",
        "\n",
        "class FPN(nn.Module):\n",
        "\n",
        "    def __init__(self, norm_layer, num_filters=128, pretrained=True):\n",
        "        \"\"\"Creates an `FPN` instance for feature extraction.\n",
        "        Args:\n",
        "          num_filters: the number of filters in each output pyramid level\n",
        "          pretrained: use ImageNet pre-trained backbone feature extractor\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        net = MobileNetV2(n_class=1000)\n",
        "\n",
        "        if pretrained:\n",
        "            #Load weights into the project directory\n",
        "            state_dict = torch.load('/content/Colab-DeblurGANv2/models/mobilenet_v2.pth.tar') # add map_location='cpu' if no gpu\n",
        "            net.load_state_dict(state_dict)\n",
        "        self.features = net.features\n",
        "\n",
        "        self.enc0 = nn.Sequential(*self.features[0:2])\n",
        "        self.enc1 = nn.Sequential(*self.features[2:4])\n",
        "        self.enc2 = nn.Sequential(*self.features[4:7])\n",
        "        self.enc3 = nn.Sequential(*self.features[7:11])\n",
        "        self.enc4 = nn.Sequential(*self.features[11:16])\n",
        "\n",
        "        self.td1 = nn.Sequential(nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1),\n",
        "                                 norm_layer(num_filters),\n",
        "                                 nn.ReLU(inplace=True))\n",
        "        self.td2 = nn.Sequential(nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1),\n",
        "                                 norm_layer(num_filters),\n",
        "                                 nn.ReLU(inplace=True))\n",
        "        self.td3 = nn.Sequential(nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1),\n",
        "                                 norm_layer(num_filters),\n",
        "                                 nn.ReLU(inplace=True))\n",
        "\n",
        "        self.lateral4 = nn.Conv2d(160, num_filters, kernel_size=1, bias=False)\n",
        "        self.lateral3 = nn.Conv2d(64, num_filters, kernel_size=1, bias=False)\n",
        "        self.lateral2 = nn.Conv2d(32, num_filters, kernel_size=1, bias=False)\n",
        "        self.lateral1 = nn.Conv2d(24, num_filters, kernel_size=1, bias=False)\n",
        "        self.lateral0 = nn.Conv2d(16, num_filters // 2, kernel_size=1, bias=False)\n",
        "\n",
        "        for param in self.features.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def unfreeze(self):\n",
        "        for param in self.features.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Bottom-up pathway, from ResNet\n",
        "        enc0 = self.enc0(x)\n",
        "\n",
        "        enc1 = self.enc1(enc0) # 256\n",
        "\n",
        "        enc2 = self.enc2(enc1) # 512\n",
        "\n",
        "        enc3 = self.enc3(enc2) # 1024\n",
        "\n",
        "        enc4 = self.enc4(enc3) # 2048\n",
        "\n",
        "        # Lateral connections\n",
        "\n",
        "        lateral4 = self.lateral4(enc4)\n",
        "        lateral3 = self.lateral3(enc3)\n",
        "        lateral2 = self.lateral2(enc2)\n",
        "        lateral1 = self.lateral1(enc1)\n",
        "        lateral0 = self.lateral0(enc0)\n",
        "\n",
        "        # Top-down pathway\n",
        "        map4 = lateral4\n",
        "        map3 = self.td1(lateral3 + nn.functional.upsample(map4, scale_factor=2, mode=\"nearest\"))\n",
        "        map2 = self.td2(lateral2 + nn.functional.upsample(map3, scale_factor=2, mode=\"nearest\"))\n",
        "        map1 = self.td3(lateral1 + nn.functional.upsample(map2, scale_factor=2, mode=\"nearest\"))\n",
        "        return lateral0, map1, map2, map3, map4\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting /content/Colab-DeblurGANv2/models/fpn_mobilenet.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSJm1AEVLu7a",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "72e636a2-9532-44a3-f7ac-c3e819aacb86"
      },
      "source": [
        "#@title Downloading model from my Google Drive (Very fast download)\n",
        "!mkdir /root/\n",
        "!mkdir /root/.cache/\n",
        "!mkdir /root/.cache/torch/\n",
        "!mkdir /root/.cache/torch/checkpoints/\n",
        "%cd /root/.cache/torch/checkpoints/\n",
        "!gdown --id 1y6GeaoWjhqjRjrXuZvCYEQYlblZGkE6X"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/root/’: File exists\n",
            "mkdir: cannot create directory ‘/root/.cache/’: File exists\n",
            "mkdir: cannot create directory ‘/root/.cache/torch/’: File exists\n",
            "mkdir: cannot create directory ‘/root/.cache/torch/checkpoints/’: File exists\n",
            "/root/.cache/torch/checkpoints\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1y6GeaoWjhqjRjrXuZvCYEQYlblZGkE6X\n",
            "To: /root/.cache/torch/checkpoints/inceptionresnetv2-520b38e4.pth\n",
            "224MB [00:01, 209MB/s]\n",
            "/content/Colab-DeblurGANv2/models\n",
            "--2020-07-04 23:56:37--  http://sceneparsing.csail.mit.edu/model/pretrained_resnet/mobilenet_v2.pth.tar\n",
            "Resolving sceneparsing.csail.mit.edu (sceneparsing.csail.mit.edu)... 128.30.100.255\n",
            "Connecting to sceneparsing.csail.mit.edu (sceneparsing.csail.mit.edu)|128.30.100.255|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14205652 (14M) [application/x-tar]\n",
            "Saving to: ‘mobilenet_v2.pth.tar.1’\n",
            "\n",
            "mobilenet_v2.pth.ta 100%[===================>]  13.55M  10.7MB/s    in 1.3s    \n",
            "\n",
            "2020-07-04 23:56:38 (10.7 MB/s) - ‘mobilenet_v2.pth.tar.1’ saved [14205652/14205652]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZwavOot5Hbm",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title ```[OPTIONAL / ONLY RUN THIS IF YOU HAVE PROBLEMS WITH THE PREVIOUS CELL]``` Testrun to download model file with pretrainedmodels (Download takes around 10 minutes)\n",
        "%cd /content/\n",
        "!wget --no-check-certificate https://raw.githubusercontent.com/xinntao/ESRGAN/master/LR/baboon.png\n",
        "%cd /content/Colab-DeblurGANv2\n",
        "!python predict.py /content/baboon.png\n",
        "\n",
        "#%cd /content/DeblurGANv2/models\n",
        "#!gdown --id 1JhnT4BBeKBBSLqTo6UsJ13HeBXevarrU\n",
        "%cd /root/.cache/torch/checkpoints\n",
        "!gdown --id 1JhnT4BBeKBBSLqTo6UsJ13HeBXevarrU\n",
        "%cd /content/Colab-DeblurGANv2/models\n",
        "!wget --no-check-certificate http://sceneparsing.csail.mit.edu/model/pretrained_resnet/mobilenet_v2.pth.tar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-rlVgQVv4lz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "cellView": "form",
        "outputId": "2ba428df-6400-4af5-95a9-7ba418167847"
      },
      "source": [
        "#@title Run deblur and copy result go Google Drive\n",
        "%cd /content/Colab-DeblurGANv2\n",
        "!python predict.py \"/content/drive/My Drive/Colab-DeblurGanv2/input.png\"\n",
        "!cp /content/Colab-DeblurGANv2/submit/input.png \"/content/drive/My Drive/Colab-DeblurGanv2/output.png\""
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Colab-DeblurGANv2\n",
            "  0% 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2796: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
            "100% 1/1 [00:00<00:00,  1.56it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}